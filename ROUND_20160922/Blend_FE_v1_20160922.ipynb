{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "\n",
    "# model libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb \n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# cross validation and grid libraries \n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "# plotting libraries in case we need it \n",
    "from matplotlib import pyplot as plt \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Intro \n",
    "This notebook is about to create Blender models, which includes: \n",
    "- **Layer1** model selection with grid search \n",
    "    - ~~Holdout % optimization with grid~~ <- done previously (60-40)\n",
    "- **Layer2** model selection with grid search \n",
    "    - Holdout % optimization with grid\n",
    "    - for dataset including layer1 predictions only \n",
    "    - for dataset including layer1 predictions and original dataset as well\n",
    "- **Test for layer3**, which is using predictions from layer2 as train set and a portion of the holdout set for test\n",
    "\n",
    "** Models: ** \n",
    "- Logistic regression\n",
    "- Nearest neighbours\n",
    "- Random forest \n",
    "- Xgboost \n",
    "- Naive Bayes with bernoulli \n",
    "- Adaboost\n",
    "- Extra trees classifier\n",
    "- Support vector Classifier \n",
    "- Quadratic discriminance analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "# Layer1 grid search \n",
    "\n",
    "With layer-holdout 60-40% split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load and split data\n",
    "train = pd.read_csv('Data/FE_v1_train.csv')\n",
    "test = pd.read_csv('Data/FE_v1_test.csv')\n",
    "train_y = pd.read_csv('Data/train_y.csv')\n",
    "\n",
    "splitter = StratifiedShuffleSplit(y=train_y, n_iter=1, train_size=0.6, \n",
    "                                      test_size=0.4, random_state=42)\n",
    "\n",
    "for train_index, holdout_index in splitter: \n",
    "    train_layer1 = train.iloc[train_index, :]\n",
    "    train_layer1.reset_index(drop=True, inplace=True)\n",
    "    train_y_layer1 = train_y.iloc[train_index, :]\n",
    "    train_y_layer1.reset_index(drop=True, inplace=True)\n",
    "    train_l1_holdout = train.iloc[holdout_index, :]\n",
    "    train_l1_holdout.reset_index(drop=True, inplace=True)\n",
    "    train_y_l1_holdout = train_y.iloc[holdout_index, :]\n",
    "    train_y_l1_holdout.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine model parameters: \n",
    "params_logreg = {'n_jobs':[4],\n",
    "                 'C':[0.0001,0.1,0.5,1.5,2,5,10], \n",
    "                 'fit_intercept':[False,True], \n",
    "                 'max_iter':[10,100,500,1000,2000],\n",
    "                 'solver':['newton-cg','lbfgs','liblinear','sag'],\n",
    "                 'tol':[0.00001,0.0001,0.001,0.1,0.5],\n",
    "                 'verbose':[0],\n",
    "                 'random_state':[42]}\n",
    "\n",
    "params_knn = {'n_jobs':[4],\n",
    "              'n_neighbors':[5,50,100,500,1000,1500,2000,3000,4000,5000], \n",
    "              'p':[1,2,3,4,5,10,11,12,15,20],\n",
    "              'leaf_size':[10,20,30,40,50,60],\n",
    "              'algorithm':['auto','ball_tree','kd_tree','brute']}\n",
    "\n",
    "params_rf = {'n_jobs':[4], \n",
    "             'criterion':['gini', 'entropy'],\n",
    "             'n_estimators':[250,500,1000,1500,2000,3000,4000,5000], \n",
    "             'max_features':[1,5,10,15,20,25], \n",
    "             'max_depth':[1,5,10,15,20,25,50,100], \n",
    "             'min_samples_split':[1,5,10,25,50], \n",
    "             'min_samples_leaf':[1,5,10,25,50], \n",
    "             'oob_score':[True,False], \n",
    "             'verbose':[0], \n",
    "             'random_state':[42]}\n",
    "\n",
    "params_xgb = {'silent':[1],\n",
    "              'nthread':[4], \n",
    "              'seed':[42], \n",
    "              'max_depth':[1,5,10,15,20,25],\n",
    "              'subsample':[0.2,0.5,0.7,1],\n",
    "              'reg_lambda':[1,2,5,10],\n",
    "              'learning_rate':[0.0001,0.001,0.01,0.1,0.3], \n",
    "              'gamma':[0,0.0001,0.001,0.01,0.1],\n",
    "              'n_estimators':[250,500,1000,1500,2000,3000,4000,5000]\n",
    "             }\n",
    "\n",
    "params_naive = {'alpha':[0.1,0.5,0.75,1,1.5,2,5,10,15,25,30,40,50,60,70,80,90,100,200,500], \n",
    "                'binarize':[0.1,0.5,0.75,1,1.5,2,5,10,15,25,30,40,50,60,70,80,90,100,200,500],\n",
    "                'fit_prior':[True, False], \n",
    "               }\n",
    "\n",
    "params_ada = {'n_estimators':[250,500,1000,1500,2000,3000,4000,5000],\n",
    "              'learning_rate':[0.0001,0.001,0.01,0.1,0.5,0.75,1.5,2,5,10,25,50], \n",
    "              'random_state':[42] \n",
    "             }\n",
    "\n",
    "# params_extra = {'n_jobs':[4],\n",
    "#                 'criterion':['gini', 'entropy'],\n",
    "#                 'n_estimators':[250,500,1000,1500,2000,3000,4000,5000], \n",
    "#                 'max_features':[1,5,10,15,20,25], \n",
    "#                 'max_depth':[1,5,10,15,20,25,50,100], \n",
    "#                 'min_samples_split':[1,5,10,25,50], \n",
    "#                 'min_samples_leaf':[1,5,10,25,50], \n",
    "#                 'oob_score':[True,False], \n",
    "#                 'bootstrap':[True],\n",
    "#                 'verbose':[0], \n",
    "#                 'random_state':[42]}\n",
    "\n",
    "params_svc = {'C':[0.0001,0.1,0.5,1.5,2,5,10], \n",
    "              'kernel':['linear','poly','rbf','sigmoid'], \n",
    "              'degree':[1,3,5,10,15,20,25,50], \n",
    "              'gamma':[0.0001,0.001,0.01,0.1,0.5,0.75,1,1.5,1.75,2,5,10],\n",
    "              'coef0':[0.0001,0.001,0.01,0.1,0.5,0.75,1,1.5,1.75,2,5,10],\n",
    "              'probability':[True],\n",
    "              'shrinking':[True, False],\n",
    "              'tol':[0.0001,0.00001],\n",
    "              'random_state':[42]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine models\n",
    "model_logreg = LogisticRegression()\n",
    "model_knn = KNeighborsClassifier()\n",
    "model_rf = RandomForestClassifier()\n",
    "model_xgb = xgb.XGBClassifier()\n",
    "model_naive = BernoulliNB()\n",
    "model_ada = AdaBoostClassifier()\n",
    "model_extra = ExtraTreesClassifier()\n",
    "model_svc = SVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed: 10.5min\n",
      "[Parallel(n_jobs=2)]: Done 1796 tasks      | elapsed: 14.8min\n",
      "[Parallel(n_jobs=2)]: Done 2446 tasks      | elapsed: 20.1min\n",
      "[Parallel(n_jobs=2)]: Done 2500 out of 2500 | elapsed: 20.6min finished\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:1207: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for (class_, warm_start_coef_) in zip(classes_, warm_start_coef))\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:1207: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for (class_, warm_start_coef_) in zip(classes_, warm_start_coef))\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/utils/optimize.py:200: UserWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"newton-cg failed to converge. Increase the \"\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/scipy/optimize/linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/utils/optimize.py:193: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/utils/optimize.py:200: UserWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"newton-cg failed to converge. Increase the \"\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params={}, iid=True, n_iter=500, n_jobs=2,\n",
       "          param_distributions={'C': [0.0001, 0.1, 0.5, 1.5, 2, 5, 10], 'n_jobs': [4], 'verbose': [0], 'tol': [1e-05, 0.0001, 0.001, 0.1, 0.5], 'fit_intercept': [False, True], 'random_state': [42], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag'], 'max_iter': [10, 100, 500, 1000, 2000]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='log_loss', verbose=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression\n",
    "grid_logreg = RandomizedSearchCV(n_iter = 500, estimator=model_logreg, param_distributions=params_logreg, \n",
    "                                 n_jobs=2, cv=5, refit=True, verbose=1, scoring='log_loss')\n",
    "\n",
    "grid_logreg.fit(train_layer1, train_y_layer1.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed: 89.0min\n",
      "[Parallel(n_jobs=2)]: Done  50 out of  50 | elapsed: 117.5min finished\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:1060: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for s in gen_even_slices(Y.shape[0], n_jobs))\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:1060: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for s in gen_even_slices(Y.shape[0], n_jobs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=2,\n",
       "          param_distributions={'n_neighbors': [5, 50, 100, 500, 1000, 1500, 2000, 3000, 4000, 5000], 'n_jobs': [4], 'leaf_size': [10, 20, 30, 40, 50, 60], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], 'p': [1, 2, 3, 4, 5, 10, 11, 12, 15, 20]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='log_loss', verbose=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Knn\n",
    "grid_knn = RandomizedSearchCV(n_iter=10, estimator=model_knn, param_distributions=params_knn, n_jobs=2, cv=5, \n",
    "                             refit=True, verbose=1, scoring='log_loss')\n",
    "\n",
    "grid_knn.fit(train_layer1, train_y_layer1.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed: 275.5min\n",
      "[Parallel(n_jobs=2)]: Done  50 out of  50 | elapsed: 291.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=2,\n",
       "          param_distributions={'n_estimators': [250, 500, 1000, 1500, 2000, 3000, 4000, 5000], 'min_samples_split': [1, 5, 10, 25, 50], 'oob_score': [True, False], 'n_jobs': [4], 'criterion': ['gini', 'entropy'], 'verbose': [0], 'max_features': [1, 5, 10, 15, 20, 25], 'random_state': [42], 'max_depth': [1, 5, 10, 15, 20, 25, 50, 100], 'min_samples_leaf': [1, 5, 10, 25, 50]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='log_loss', verbose=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "grid_rf = RandomizedSearchCV(estimator=model_rf, param_distributions=params_rf, n_jobs=2, cv=5, refit=True, \n",
    "                             verbose=1, scoring='log_loss')\n",
    "\n",
    "grid_rf.fit(train_layer1, train_y_layer1.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  52 tasks      | elapsed:    1.1s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    3.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       "          fit_params={}, iid=True, n_iter=40, n_jobs=8,\n",
       "          param_distributions={'binarize': [0.1, 0.5, 0.75, 1, 1.5, 2, 5, 10, 15, 25, 30, 40, 50, 60, 70, 80, 90, 100, 200, 500], 'alpha': [0.1, 0.5, 0.75, 1, 1.5, 2, 5, 10, 15, 25, 30, 40, 50, 60, 70, 80, 90, 100, 200, 500], 'fit_prior': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='log_loss', verbose=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bernoulli Bayes\n",
    "grid_naive = RandomizedSearchCV(n_iter=40, estimator=model_naive, param_distributions=params_naive, n_jobs=8, \n",
    "                                cv=5, refit=True, verbose=1, scoring='log_loss')\n",
    "grid_naive.fit(train_layer1, train_y_layer1.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Adaboost\n",
    "grid_ada = RandomizedSearchCV(n_iter=15, estimator=model_ada, param_distributions=params_ada, n_jobs=8, \n",
    "                              cv=5, refit=True, verbose=1, scoring='log_loss')\n",
    "grid_ada.fit(train_layer1, train_y_layer1.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# XGB\n",
    "splitter = StratifiedShuffleSplit(y=train_y_layer1, n_iter=1, train_size=0.7, \n",
    "                                      test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, test_index in splitter: \n",
    "    train_l1_1 = train_layer1.iloc[train_index,:]\n",
    "    train_l1_1.reset_index(drop=True, inplace=True)\n",
    "    train_y_l1_1 = train_y_layer1.iloc[train_index,:]\n",
    "    train_y_l1_1.reset_index(drop=True, inplace=True)\n",
    "    train_l1_2 = train_layer1.iloc[test_index,:]\n",
    "    train_l1_2.reset_index(drop=True, inplace=True)\n",
    "    train_y_l1_2 = train_y_layer1.iloc[test_index,:]\n",
    "    train_l1_2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "xgb_fit_params = {'eval_set':[(train_l1_1, train_y_l1_1),(train_l1_2, train_y_l1_2)], \n",
    "                  'early_stopping_rounds':25, 'verbose':0}\n",
    "\n",
    "grid_xgb = RandomizedSearchCV(n_iter=20, estimator=model_xgb, param_distributions=params_xgb, \n",
    "                              n_jobs=2, cv=5, refit=True, verbose=1, scoring='log_loss', \n",
    "                              fit_params=xgb_fit_params\n",
    "                             )\n",
    "\n",
    "grid_xgb.fit(train_layer1, train_y_layer1.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logreg:  {'fit_intercept': False, 'C': 0.1, 'n_jobs': 4, 'verbose': 0, 'solver': 'sag', 'max_iter': 1000, 'random_state': 42, 'tol': 0.001} -0.691717645023\n",
      "KNN:  {'p': 2, 'n_jobs': 4, 'leaf_size': 20, 'algorithm': 'ball_tree', 'n_neighbors': 1000} -0.692166108114\n",
      "Bernoulli bayes:  {'binarize': 10, 'alpha': 1.5, 'fit_prior': True} -0.692666541942\n"
     ]
    }
   ],
   "source": [
    "# these are results from bigdell rerun\n",
    "print 'Logreg: ', grid_logreg.best_params_, grid_logreg.best_score_\n",
    "print 'KNN: ', grid_knn.best_params_, grid_knn.best_score_\n",
    "# print 'RandomForest: ', grid_rf.best_params_, grid_rf.best_score_\n",
    "print 'Bernoulli bayes: ', grid_naive.best_params_, grid_naive.best_score_\n",
    "# print 'Adaboost: ', grid_ada.best_params_, grid_ada.best_score_\n",
    "# print 'Xgboost: ', grid_xgb.best_params_, grid_xgb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Logreg:  {'fit_intercept': False, 'C': 0.1, 'n_jobs': 4, 'verbose': 0, 'solver': 'sag', 'max_iter': 100, 'random_state': 42, 'tol': 0.001} -0.691717645023\n",
      "KNN:  {'p': 1, 'n_jobs': 4, 'leaf_size': 20, 'algorithm': 'ball_tree', 'n_neighbors': 4000} -0.692126133875\n",
      "RandomForest:  {'oob_score': False, 'n_jobs': 4, 'verbose': 0, 'min_samples_leaf': 25, 'n_estimators': 1500, 'max_features': 15, 'random_state': 42, 'criterion': 'entropy', 'min_samples_split': 50, 'max_depth': 10} -0.692124787781\n",
      "Bernoulli bayes:  {'binarize': 15, 'alpha': 0.5, 'fit_prior': True} -0.693057262966\n",
      "Adaboost:  {'n_estimators': 5000, 'learning_rate': 0.0001, 'random_state': 42} -0.692654655439\n",
      "Xgboost:  {'silent': 1, 'learning_rate': 0.1, 'nthread': 4, 'n_estimators': 5000, 'subsample': 0.7, 'reg_lambda': 5, 'seed': 42, 'max_depth': 1, 'gamma': 0.0001} -0.691804040637\n"
     ]
    }
   ],
   "source": [
    "# these are results from littleDell\n",
    "print 'Logreg: ', grid_logreg.best_params_, grid_logreg.best_score_\n",
    "print 'KNN: ', grid_knn.best_params_, grid_knn.best_score_\n",
    "print 'RandomForest: ', grid_rf.best_params_, grid_rf.best_score_\n",
    "print 'Bernoulli bayes: ', grid_naive.best_params_, grid_naive.best_score_\n",
    "print 'Adaboost: ', grid_ada.best_params_, grid_ada.best_score_\n",
    "print 'Xgboost: ', grid_xgb.best_params_, grid_xgb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print out results into GridResults/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I had some problems with the following algos: \n",
    "\n",
    "# Extra trees - parameter error maybe?\n",
    "grid_extra = RandomizedSearchCV(estimator=model_extra, param_distributions=params_extra, n_jobs=1, cv=5,\n",
    "                               refit=True, verbose=1, scoring='log_loss')\n",
    "grid_extra.fit(train_layer1, train_y_layer1.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    }
   ],
   "source": [
    "# support vector classifier - takes too much time, on bigdell one night is not enough\n",
    "grid_svc = RandomizedSearchCV(estimator=model_svc, param_distributions=params_svc, n_jobs=8, cv=5, \n",
    "                             refit=True, verbose=1, scoring='log_loss')\n",
    "\n",
    "grid_svc.fit(train_layer1, train_y_layer1.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blend_v1\n",
    "## Create predictions with layer1 models on holdout set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set model parameters\n",
    "params_logreg = {'n_jobs':4,\n",
    "                 'C':0.1, \n",
    "                 'fit_intercept':False, \n",
    "                 'max_iter':100,\n",
    "                 'solver':'sag',\n",
    "                 'tol':0.001,\n",
    "                 'verbose':0,\n",
    "                 'random_state':42\n",
    "                }\n",
    "\n",
    "params_knn = {'n_jobs':4,\n",
    "              'n_neighbors':4000, \n",
    "              'p':1,\n",
    "              'leaf_size':20,\n",
    "              'algorithm':'ball_tree'\n",
    "             }\n",
    "\n",
    "params_rf = {'n_jobs':4, \n",
    "             'criterion':'entropy',\n",
    "             'n_estimators':1500, \n",
    "             'max_features':15, \n",
    "             'max_depth':10,\n",
    "             'min_samples_split':50, \n",
    "             'min_samples_leaf':25, \n",
    "             'oob_score':False, \n",
    "             'verbose':0,\n",
    "             'random_state':42\n",
    "            }\n",
    "\n",
    "params_xgb = {'silent':1,\n",
    "              'nthread':4, \n",
    "              'seed':42, \n",
    "              'max_depth':1,\n",
    "              'subsample':0.7,\n",
    "              'reg_lambda':5,\n",
    "              'learning_rate':0.1, \n",
    "              'gamma':0.0001,\n",
    "              'n_estimators':5000\n",
    "             }\n",
    "\n",
    "params_naive = {'alpha':0.5,\n",
    "                'binarize':15,\n",
    "                'fit_prior':True \n",
    "               }\n",
    "\n",
    "params_ada = {'n_estimators':5000,\n",
    "              'learning_rate':0.0001, \n",
    "              'random_state':42 \n",
    "             }\n",
    "\n",
    "# Set models\n",
    "model_logreg = LogisticRegression(**params_logreg)\n",
    "model_knn = KNeighborsClassifier(**params_knn)\n",
    "model_rf = RandomForestClassifier(**params_rf)\n",
    "model_xgb = xgb.XGBClassifier(**params_xgb)\n",
    "model_naive = BernoulliNB(**params_naive)\n",
    "model_ada = AdaBoostClassifier(**params_ada)\n",
    "# model_extra = ExtraTreesClassifier()\n",
    "# model_svc = SVC() -> takes too much time for randomGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logreg done...\n",
      "knn done...\n",
      "rf done...\n",
      "naive done...\n",
      "ada done...\n"
     ]
    }
   ],
   "source": [
    "# create predictions for holdout set\n",
    "model_logreg.fit(train_layer1, train_y_layer1.target)\n",
    "print 'logreg done...'\n",
    "model_knn.fit(train_layer1, train_y_layer1.target)\n",
    "print 'knn done...'\n",
    "model_rf.fit(train_layer1, train_y_layer1.target)\n",
    "print 'rf done...'\n",
    "model_naive.fit(train_layer1, train_y_layer1.target)\n",
    "print 'naive done...'\n",
    "model_ada.fit(train_layer1, train_y_layer1.target)\n",
    "print 'ada done...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgb done...\n"
     ]
    }
   ],
   "source": [
    "#xgboost\n",
    "# XGB\n",
    "splitter = StratifiedShuffleSplit(y=train_y_layer1, n_iter=1, train_size=0.7, \n",
    "                                      test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, test_index in splitter: \n",
    "    train_l1_1 = train_layer1.iloc[train_index,:]\n",
    "    train_l1_1.reset_index(drop=True, inplace=True)\n",
    "    train_y_l1_1 = train_y_layer1.iloc[train_index,:]\n",
    "    train_y_l1_1.reset_index(drop=True, inplace=True)\n",
    "    train_l1_2 = train_layer1.iloc[test_index,:]\n",
    "    train_l1_2.reset_index(drop=True, inplace=True)\n",
    "    train_y_l1_2 = train_y_layer1.iloc[test_index,:]\n",
    "    train_l1_2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "model_xgb.fit(train_layer1, train_y_layer1.target,\n",
    "            eval_set=[(train_l1_1, train_y_l1_1),(train_l1_2, train_y_l1_2)], \n",
    "            early_stopping_rounds=25, verbose=0)\n",
    "print 'xgb done...'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict on holdout set\n",
    "holdout_predict_logreg = model_logreg.predict_proba(train_l1_holdout)\n",
    "holdout_predict_knn = model_knn.predict_proba(train_l1_holdout)\n",
    "holdout_predict_rf = model_rf.predict_proba(train_l1_holdout)\n",
    "holdout_predict_naive = model_naive.predict_proba(train_l1_holdout)\n",
    "holdout_predict_ada = model_ada.predict_proba(train_l1_holdout)\n",
    "holdout_predict_xgb = model_xgb.predict_proba(train_l1_holdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logloss 0.691567060898\n",
      "knn 0.691726372791\n",
      "rf 0.691702025755\n",
      "naive 0.693022286855\n",
      "ada 0.692435168301\n",
      "xgb 0.691709935212\n"
     ]
    }
   ],
   "source": [
    "# Calculate error on holdout set\n",
    "print 'logloss', metrics.log_loss(y_true=train_y_l1_holdout, y_pred=holdout_predict_logreg[:,1])\n",
    "print 'knn', metrics.log_loss(y_true=train_y_l1_holdout, y_pred=holdout_predict_knn[:,1])\n",
    "print 'rf', metrics.log_loss(y_true=train_y_l1_holdout, y_pred=holdout_predict_rf[:,1])\n",
    "print 'naive', metrics.log_loss(y_true=train_y_l1_holdout, y_pred=holdout_predict_naive[:,1])\n",
    "print 'ada', metrics.log_loss(y_true=train_y_l1_holdout, y_pred=holdout_predict_ada[:,1])\n",
    "print 'xgb', metrics.log_loss(y_true=train_y_l1_holdout, y_pred=holdout_predict_xgb[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive bayes and adaboost have a poor performance, the others are ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict on test set\n",
    "test_predict_logreg = model_logreg.predict_proba(test.iloc[:,1])\n",
    "test_predict_knn = model_knn.predict_proba(test.iloc[:,1])\n",
    "test_predict_rf = model_rf.predict_proba(test.iloc[:,1])\n",
    "test_predict_naive = model_naive.predict_proba(test.iloc[:,1])\n",
    "test_predict_ada = model_ada.predict_proba(test.iloc[:,1])\n",
    "test_predict_xgb = model_xgb.predict_proba(test.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# concat holdout with predictions \n",
    "temp = pd.concat([pd.DataFrame(holdout_predict_logreg[:,1]),\n",
    "                 pd.DataFrame(holdout_predict_knn[:,1]), \n",
    "                 pd.DataFrame(holdout_predict_rf[:,1]),\n",
    "                 pd.DataFrame(holdout_predict_naive[:,1]),\n",
    "                 pd.DataFrame(holdout_predict_ada[:,1]), \n",
    "                 pd.DataFrame(holdout_predict_xgb[:,1])], axis=1)\n",
    "temp.columns = ['pred_logreg', 'pred_knn', 'pred_rf', 'pred_naive', 'pred_ada', 'pred_xgb']\n",
    "\n",
    "train_l1_holdout_blend_v1 = pd.concat([train_l1_holdout, temp], axis=1)\n",
    "\n",
    "\n",
    "# concat test with predictions\n",
    "temp = pd.concat([pd.DataFrame(test_predict_logreg[:,1]),\n",
    "                 pd.DataFrame(test_predict_knn[:,1]), \n",
    "                 pd.DataFrame(test_predict_rf[:,1]),\n",
    "                 pd.DataFrame(test_predict_naive[:,1]),\n",
    "                 pd.DataFrame(test_predict_ada[:,1]), \n",
    "                 pd.DataFrame(test_predict_xgb[:,1])], axis=1)\n",
    "temp.columns = ['pred_logreg', 'pred_knn', 'pred_rf', 'pred_naive', 'pred_ada', 'pred_xgb']\n",
    "\n",
    "test_blend_v1 = pd.concat([test, temp], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save tables in csv files\n",
    "pd.DataFrame.to_csv(train_layer1, 'Blend_data/Blend_v1_train_layer1.csv', index=False)\n",
    "pd.DataFrame.to_csv(train_y_layer1, 'Blend_data/Blend_v1_train_y_layer1.csv', index=False)\n",
    "pd.DataFrame.to_csv(train_l1_holdout_blend_v1, 'Blend_data/Blend_v1_train_l1_holdout.csv', index=False)\n",
    "pd.DataFrame.to_csv(train_y_l1_holdout, 'Blend_data/Blend_v1_train_y_holdout.csv', index=False)\n",
    "\n",
    "pd.DataFrame.to_csv(test_blend_v1, 'Blend_data/Blend_v1_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search for blender model WITH original data and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_l1_holdout_blend_v1 = pd.read_csv('Blend_data/Blend_v1_train_l1_holdout.csv')\n",
    "train_y_l1_holdout = pd.read_csv('Blend_data/Blend_v1_train_y_holdout.csv')\n",
    "\n",
    "test_blend_v1 = pd.read_csv('Blend_data/Blend_v1_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine model parameters: \n",
    "params_logreg = {'n_jobs':[4],\n",
    "                 'C':[0.0001,0.1,0.5,1.5,2,5,10], \n",
    "                 'fit_intercept':[False,True], \n",
    "                 'max_iter':[10,100,500,1000,2000],\n",
    "                 'solver':['newton-cg','lbfgs','liblinear','sag'],\n",
    "                 'tol':[0.00001,0.0001,0.001,0.1,0.5],\n",
    "                 'verbose':[0],\n",
    "                 'random_state':[42]}\n",
    "\n",
    "params_knn = {'n_jobs':[4],\n",
    "              'n_neighbors':[5,50,100,500,1000,1500,2000,3000,4000,5000], \n",
    "              'p':[1,2,3,4,5,10,11,12,15,20],\n",
    "              'leaf_size':[10,20,30,40,50,60],\n",
    "              'algorithm':['auto','ball_tree','kd_tree','brute']}\n",
    "\n",
    "params_rf = {'n_jobs':[4], \n",
    "             'criterion':['gini', 'entropy'],\n",
    "             'n_estimators':[250,500,1000,1500,2000,3000,4000,5000], \n",
    "             'max_features':[1,5,10,15,20,25], \n",
    "             'max_depth':[1,5,10,15,20,25,50,100], \n",
    "             'min_samples_split':[1,5,10,25,50], \n",
    "             'min_samples_leaf':[1,5,10,25,50], \n",
    "             'oob_score':[True,False], \n",
    "             'verbose':[0], \n",
    "             'random_state':[42]}\n",
    "\n",
    "params_xgb = {'silent':[1],\n",
    "              'nthread':[4], \n",
    "              'seed':[42], \n",
    "              'max_depth':[1,5,10,15,20,25],\n",
    "              'subsample':[0.2,0.5,0.7,1],\n",
    "              'reg_lambda':[1,2,5,10],\n",
    "              'learning_rate':[0.0001,0.001,0.01,0.1,0.3], \n",
    "              'gamma':[0,0.0001,0.001,0.01,0.1],\n",
    "              'n_estimators':[250,500,1000,1500,2000,3000,4000,5000]\n",
    "             }\n",
    "\n",
    "params_naive = {'alpha':[0.1,0.5,0.75,1,1.5,2,5,10,15,25,30,40,50,60,70,80,90,100,200,500], \n",
    "                'binarize':[0.1,0.5,0.75,1,1.5,2,5,10,15,25,30,40,50,60,70,80,90,100,200,500],\n",
    "                'fit_prior':[True, False], \n",
    "               }\n",
    "\n",
    "params_ada = {'n_estimators':[250,500,1000,1500,2000,3000,4000,5000],\n",
    "              'learning_rate':[0.0001,0.001,0.01,0.1,0.5,0.75,1.5,2,5,10,25,50], \n",
    "              'random_state':[42] \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine models\n",
    "model_logreg = LogisticRegression()\n",
    "model_knn = KNeighborsClassifier()\n",
    "model_rf = RandomForestClassifier()\n",
    "model_xgb = xgb.XGBClassifier()\n",
    "model_naive = BernoulliNB()\n",
    "model_ada = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:   30.2s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed: 17.0min\n",
      "[Parallel(n_jobs=2)]: Done 1796 tasks      | elapsed: 23.3min\n",
      "[Parallel(n_jobs=2)]: Done 2446 tasks      | elapsed: 32.5min\n",
      "[Parallel(n_jobs=2)]: Done 2500 out of 2500 | elapsed: 33.8min finished\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:1207: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for (class_, warm_start_coef_) in zip(classes_, warm_start_coef))\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:1207: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for (class_, warm_start_coef_) in zip(classes_, warm_start_coef))\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/utils/optimize.py:200: UserWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"newton-cg failed to converge. Increase the \"\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/utils/optimize.py:200: UserWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"newton-cg failed to converge. Increase the \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params={}, iid=True, n_iter=500, n_jobs=2,\n",
       "          param_distributions={'C': [0.0001, 0.1, 0.5, 1.5, 2, 5, 10], 'n_jobs': [4], 'verbose': [0], 'tol': [1e-05, 0.0001, 0.001, 0.1, 0.5], 'fit_intercept': [False, True], 'random_state': [42], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag'], 'max_iter': [10, 100, 500, 1000, 2000]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='log_loss', verbose=1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression\n",
    "grid_logreg = RandomizedSearchCV(n_iter = 500, estimator=model_logreg, param_distributions=params_logreg, \n",
    "                                 n_jobs=2, cv=5, refit=True, verbose=1, scoring='log_loss')\n",
    "\n",
    "grid_logreg.fit(train_l1_holdout_blend_v1, train_y_l1_holdout.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed: 35.1min\n",
      "[Parallel(n_jobs=2)]: Done  50 out of  50 | elapsed: 38.2min finished\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:1060: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for s in gen_even_slices(Y.shape[0], n_jobs))\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:1060: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for s in gen_even_slices(Y.shape[0], n_jobs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=2,\n",
       "          param_distributions={'n_neighbors': [5, 50, 100, 500, 1000, 1500, 2000, 3000, 4000, 5000], 'n_jobs': [4], 'leaf_size': [10, 20, 30, 40, 50, 60], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], 'p': [1, 2, 3, 4, 5, 10, 11, 12, 15, 20]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='log_loss', verbose=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Knn\n",
    "grid_knn = RandomizedSearchCV(n_iter=10, estimator=model_knn, param_distributions=params_knn, n_jobs=2, cv=5, \n",
    "                             refit=True, verbose=1, scoring='log_loss')\n",
    "\n",
    "grid_knn.fit(train_l1_holdout_blend_v1, train_y_l1_holdout.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed: 115.9min\n",
      "[Parallel(n_jobs=2)]: Done  50 out of  50 | elapsed: 123.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=2,\n",
       "          param_distributions={'n_estimators': [250, 500, 1000, 1500, 2000, 3000, 4000, 5000], 'min_samples_split': [1, 5, 10, 25, 50], 'oob_score': [True, False], 'n_jobs': [4], 'criterion': ['gini', 'entropy'], 'verbose': [0], 'max_features': [1, 5, 10, 15, 20, 25], 'random_state': [42], 'max_depth': [1, 5, 10, 15, 20, 25, 50, 100], 'min_samples_leaf': [1, 5, 10, 25, 50]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='log_loss', verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "grid_rf = RandomizedSearchCV(estimator=model_rf, param_distributions=params_rf, n_jobs=2, cv=5, refit=True, \n",
    "                             verbose=1, scoring='log_loss')\n",
    "\n",
    "grid_rf.fit(train_l1_holdout_blend_v1, train_y_l1_holdout.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  88 tasks      | elapsed:    1.3s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    2.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       "          fit_params={}, iid=True, n_iter=40, n_jobs=8,\n",
       "          param_distributions={'binarize': [0.1, 0.5, 0.75, 1, 1.5, 2, 5, 10, 15, 25, 30, 40, 50, 60, 70, 80, 90, 100, 200, 500], 'alpha': [0.1, 0.5, 0.75, 1, 1.5, 2, 5, 10, 15, 25, 30, 40, 50, 60, 70, 80, 90, 100, 200, 500], 'fit_prior': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='log_loss', verbose=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bernoulli Bayes\n",
    "grid_naive = RandomizedSearchCV(n_iter=40, estimator=model_naive, param_distributions=params_naive, n_jobs=8, \n",
    "                                cv=5, refit=True, verbose=1, scoring='log_loss')\n",
    "grid_naive.fit(train_l1_holdout_blend_v1, train_y_l1_holdout.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed: 31.9min\n"
     ]
    },
    {
     "ename": "JoblibValueError",
     "evalue": "JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    157     pkg_name = mod_name.rpartition('.')[0]\n    158     main_globals = sys.modules[\"__main__\"].__dict__\n    159     if alter_argv:\n    160         sys.argv[0] = fname\n    161     return _run_code(code, main_globals, None,\n--> 162                      \"__main__\", fname, loader, pkg_name)\n        fname = '/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py'\n        loader = <pkgutil.ImpLoader instance>\n        pkg_name = 'ipykernel'\n    163 \n    164 def run_module(mod_name, init_globals=None,\n    165                run_name=None, alter_sys=False):\n    166     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/runpy.py in _run_code(code=<code object <module> at 0x7f34d7be8eb0, file \"/...2.7/site-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/home/ngergo...python2.7/site-packages/ipykernel/kernelapp.pyc'>}, init_globals=None, mod_name='__main__', mod_fname='/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py', mod_loader=<pkgutil.ImpLoader instance>, pkg_name='ipykernel')\n     67         run_globals.update(init_globals)\n     68     run_globals.update(__name__ = mod_name,\n     69                        __file__ = mod_fname,\n     70                        __loader__ = mod_loader,\n     71                        __package__ = pkg_name)\n---> 72     exec code in run_globals\n        code = <code object <module> at 0x7f34d7be8eb0, file \"/...2.7/site-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/home/ngergo...python2.7/site-packages/ipykernel/kernelapp.pyc'>}\n     73     return run_globals\n     74 \n     75 def _run_module_code(code, init_globals=None,\n     76                     mod_name=None, mod_fname=None,\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    587         \n    588         If a global instance already exists, this reinitializes and starts it\n    589         \"\"\"\n    590         app = cls.instance(**kwargs)\n    591         app.initialize(argv)\n--> 592         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    593 \n    594 #-----------------------------------------------------------------------------\n    595 # utility functions, for convenience\n    596 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    398         \n    399         if self.poller is not None:\n    400             self.poller.start()\n    401         self.kernel.start()\n    402         try:\n--> 403             ioloop.IOLoop.instance().start()\n    404         except KeyboardInterrupt:\n    405             pass\n    406 \n    407 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    146             PollIOLoop.configure(ZMQIOLoop)\n    147         return PollIOLoop.instance()\n    148     \n    149     def start(self):\n    150         try:\n--> 151             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    152         except ZMQError as e:\n    153             if e.errno == ETERM:\n    154                 # quietly return on ETERM\n    155                 pass\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    878                 self._events.update(event_pairs)\n    879                 while self._events:\n    880                     fd, events = self._events.popitem()\n    881                     try:\n    882                         fd_obj, handler_func = self._handlers[fd]\n--> 883                         handler_func(fd_obj, events)\n        handler_func = <function null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 5\n    884                     except (OSError, IOError) as e:\n    885                         if errno_from_exception(e) == errno.EPIPE:\n    886                             # Happens when the client closes the connection\n    887                             pass\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 5), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 5)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=5)\n    428             # dispatch events:\n    429             if events & IOLoop.ERROR:\n    430                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    431                 return\n    432             if events & IOLoop.READ:\n--> 433                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    434                 if not self.socket:\n    435                     return\n    436             if events & IOLoop.WRITE:\n    437                 self._handle_send()\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    460                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    461         else:\n    462             if self._recv_callback:\n    463                 callback = self._recv_callback\n    464                 # self._recv_callback = None\n--> 465                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    466                 \n    467         # self.update_state()\n    468         \n    469 \n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    402         close our socket.\"\"\"\n    403         try:\n    404             # Use a NullContext to ensure that all StackContexts are run\n    405             # inside our blanket exception handler rather than outside.\n    406             with stack_context.NullContext():\n--> 407                 callback(*args, **kwargs)\n        callback = <function null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    408         except:\n    409             gen_log.error(\"Uncaught exception, closing connection.\",\n    410                           exc_info=True)\n    411             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    255         if self.control_stream:\n    256             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    257 \n    258         def make_dispatcher(stream):\n    259             def dispatcher(msg):\n--> 260                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    261             return dispatcher\n    262 \n    263         for s in self.shell_streams:\n    264             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {u'allow_stdin': True, u'code': u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2016-09-23T22:06:50.089407', u'msg_id': u'C42EFFEC2ADD4235956D9533428BA7C0', u'msg_type': u'execute_request', u'session': u'1422CF0855EF4AA286CC064870292C16', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'C42EFFEC2ADD4235956D9533428BA7C0', 'msg_type': u'execute_request', 'parent_header': {}})\n    207             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    208         else:\n    209             self.log.debug(\"%s: %s\", msg_type, msg)\n    210             self.pre_handler_hook()\n    211             try:\n--> 212                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = ['1422CF0855EF4AA286CC064870292C16']\n        msg = {'buffers': [], 'content': {u'allow_stdin': True, u'code': u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2016-09-23T22:06:50.089407', u'msg_id': u'C42EFFEC2ADD4235956D9533428BA7C0', u'msg_type': u'execute_request', u'session': u'1422CF0855EF4AA286CC064870292C16', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'C42EFFEC2ADD4235956D9533428BA7C0', 'msg_type': u'execute_request', 'parent_header': {}}\n    213             except Exception:\n    214                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    215             finally:\n    216                 self.post_handler_hook()\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=['1422CF0855EF4AA286CC064870292C16'], parent={'buffers': [], 'content': {u'allow_stdin': True, u'code': u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2016-09-23T22:06:50.089407', u'msg_id': u'C42EFFEC2ADD4235956D9533428BA7C0', u'msg_type': u'execute_request', u'session': u'1422CF0855EF4AA286CC064870292C16', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'C42EFFEC2ADD4235956D9533428BA7C0', 'msg_type': u'execute_request', 'parent_header': {}})\n    365         if not silent:\n    366             self.execution_count += 1\n    367             self._publish_execute_input(code, parent, self.execution_count)\n    368 \n    369         reply_content = self.do_execute(code, silent, store_history,\n--> 370                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    371 \n    372         # Flush output before sending the reply.\n    373         sys.stdout.flush()\n    374         sys.stderr.flush()\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    170 \n    171         reply_content = {}\n    172         # FIXME: the shell calls the exception handler itself.\n    173         shell._reply_content = None\n    174         try:\n--> 175             shell.run_cell(code, store_history=store_history, silent=silent)\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\"\n        store_history = True\n        silent = False\n    176         except:\n    177             status = u'error'\n    178             # FIXME: this code right now isn't being used yet by default,\n    179             # because the run_cell() call above directly fires off exception\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", store_history=True, silent=False, shell_futures=True)\n   2897                 self.displayhook.exec_result = result\n   2898 \n   2899                 # Execute the user code\n   2900                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2901                 self.run_ast_nodes(code_ast.body, cell_name,\n-> 2902                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler instance>\n   2903 \n   2904                 # Reset this so later displayed values do not modify the\n   2905                 # ExecutionResult\n   2906                 self.displayhook.exec_result = None\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Expr object>], cell_name='<ipython-input-12-48cae6f0dd09>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler instance>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   3007                     return True\n   3008 \n   3009             for i, node in enumerate(to_run_interactive):\n   3010                 mod = ast.Interactive([node])\n   3011                 code = compiler(mod, cell_name, \"single\")\n-> 3012                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f348bfd29b0, file \"<ipython-input-12-48cae6f0dd09>\", line 4>\n        result = <IPython.core.interactiveshell.ExecutionResult object>\n   3013                     return True\n   3014 \n   3015             # Flush softspace\n   3016             if softspace(sys.stdout, 0):\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f348bfd29b0, file \"<ipython-input-12-48cae6f0dd09>\", line 4>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   3061         outflag = 1  # happens in more places, so it's easier as default\n   3062         try:\n   3063             try:\n   3064                 self.hooks.pre_run_code_hook()\n   3065                 #rprint('Running code', repr(code_obj)) # dbg\n-> 3066                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f348bfd29b0, file \"<ipython-input-12-48cae6f0dd09>\", line 4>\n        self.user_global_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 'BernoulliNB': <class 'sklearn.naive_bayes.BernoulliNB'>, 'ExtraTreesClassifier': <class 'sklearn.ensemble.forest.ExtraTreesClassifier'>, 'In': ['', u\"train_l1_holdout_blend_v1 = pd.read_csv('Blend...v1 = pd.read_csv('Blend_data/Blend_v1_test.csv')\", u\"# Import libraries\\nimport pandas as pd\\n\\n# m... plt \\nget_ipython().magic(u'matplotlib inline')\", u\"train_l1_holdout_blend_v1 = pd.read_csv('Blend...v1 = pd.read_csv('Blend_data/Blend_v1_test.csv')\", u\"# Determine model parameters: \\nparams_logreg ...            'random_state':[42] \\n             }\", u'# Determine models\\nmodel_logreg = LogisticReg... BernoulliNB()\\nmodel_ada = AdaBoostClassifier()', u\"#Logistic regression\\ngrid_logreg = Randomized..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# XGB\\nsplitter = StratifiedShuffleSplit(y=tra..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"#Logistic regression\\ngrid_logreg = Randomized..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Knn\\ngrid_knn = RandomizedSearchCV(n_iter=10..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Random Forest\\ngrid_rf = RandomizedSearchCV(..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Bernoulli Bayes\\ngrid_naive = RandomizedSear..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\"], 'KNeighborsClassifier': <class 'sklearn.neighbors.classification.KNeighborsClassifier'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {7: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 8: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 9: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 10: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 11: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1)}, 'QuadraticDiscriminantAnalysis': <class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'RandomizedSearchCV': <class 'sklearn.grid_search.RandomizedSearchCV'>, ...}\n        self.user_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 'BernoulliNB': <class 'sklearn.naive_bayes.BernoulliNB'>, 'ExtraTreesClassifier': <class 'sklearn.ensemble.forest.ExtraTreesClassifier'>, 'In': ['', u\"train_l1_holdout_blend_v1 = pd.read_csv('Blend...v1 = pd.read_csv('Blend_data/Blend_v1_test.csv')\", u\"# Import libraries\\nimport pandas as pd\\n\\n# m... plt \\nget_ipython().magic(u'matplotlib inline')\", u\"train_l1_holdout_blend_v1 = pd.read_csv('Blend...v1 = pd.read_csv('Blend_data/Blend_v1_test.csv')\", u\"# Determine model parameters: \\nparams_logreg ...            'random_state':[42] \\n             }\", u'# Determine models\\nmodel_logreg = LogisticReg... BernoulliNB()\\nmodel_ada = AdaBoostClassifier()', u\"#Logistic regression\\ngrid_logreg = Randomized..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# XGB\\nsplitter = StratifiedShuffleSplit(y=tra..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"#Logistic regression\\ngrid_logreg = Randomized..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Knn\\ngrid_knn = RandomizedSearchCV(n_iter=10..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Random Forest\\ngrid_rf = RandomizedSearchCV(..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Bernoulli Bayes\\ngrid_naive = RandomizedSear..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\"], 'KNeighborsClassifier': <class 'sklearn.neighbors.classification.KNeighborsClassifier'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {7: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 8: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 9: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 10: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 11: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1)}, 'QuadraticDiscriminantAnalysis': <class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'RandomizedSearchCV': <class 'sklearn.grid_search.RandomizedSearchCV'>, ...}\n   3067             finally:\n   3068                 # Reset our crash handler in place\n   3069                 sys.excepthook = old_excepthook\n   3070         except SystemExit as e:\n\n...........................................................................\n/home/ngergoo/Documents/DataMining/Numerai/ROUND_20160922/<ipython-input-12-48cae6f0dd09> in <module>()\n      1 \n      2 \n      3 # Adaboost\n----> 4 grid_ada = RandomizedSearchCV(n_iter=10, estimator=model_ada, param_distributions=params_ada, n_jobs=8, \n      5                               cv=5, refit=True, verbose=1, scoring='log_loss')\n      6 grid_ada.fit(train_l1_holdout_blend_v1, train_y_l1_holdout.target)\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py in fit(self=RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), X=       feature1  feature2  feature3  feature4  f...  0.506965  0.499143  \n\n[38528 rows x 33 columns], y=0        0\n1        1\n2        1\n3        1\n4   ...\n38526    0\n38527    0\nName: target, dtype: int64)\n    991 \n    992         \"\"\"\n    993         sampled_params = ParameterSampler(self.param_distributions,\n    994                                           self.n_iter,\n    995                                           random_state=self.random_state)\n--> 996         return self._fit(X, y, sampled_params)\n        self._fit = <bound method RandomizedSearchCV._fit of Randomi...t=True,\n          scoring='log_loss', verbose=1)>\n        X =        feature1  feature2  feature3  feature4  f...  0.506965  0.499143  \n\n[38528 rows x 33 columns]\n        y = 0        0\n1        1\n2        1\n3        1\n4   ...\n38526    0\n38527    0\nName: target, dtype: int64\n        sampled_params = <sklearn.grid_search.ParameterSampler object>\n    997 \n    998 \n    999 \n   1000 \n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py in _fit(self=RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), X=       feature1  feature2  feature3  feature4  f...  0.506965  0.499143  \n\n[38528 rows x 33 columns], y=0        0\n1        1\n2        1\n3        1\n4   ...\n38526    0\n38527    0\nName: target, dtype: int64, parameter_iterable=<sklearn.grid_search.ParameterSampler object>)\n    548         )(\n    549             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    550                                     train, test, self.verbose, parameters,\n    551                                     self.fit_params, return_parameters=True,\n    552                                     error_score=self.error_score)\n--> 553                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterSampler object>\n    554                 for train, test in cv)\n    555 \n    556         # Out is a list of triplet: score, estimator, n_test_samples\n    557         n_fits = len(out)\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=8), iterable=<generator object <genexpr>>)\n    807             if pre_dispatch == \"all\" or n_jobs == 1:\n    808                 # The iterable was consumed all at once by the above for loop.\n    809                 # No need to wait for async callbacks to trigger to\n    810                 # consumption.\n    811                 self._iterating = False\n--> 812             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=8)>\n    813             # Make sure that we get a last message telling us we are done\n    814             elapsed_time = time.time() - self._start_time\n    815             self._print('Done %3i out of %3i | elapsed: %s finished',\n    816                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Sep 23 22:39:04 2016\nPID: 23643                Python 2.7.11: /home/ngergoo/anaconda2/bin/python\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.pyc in _fit_and_score(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_est...=50,\n          n_estimators=250, random_state=42), X=       feature1  feature2  feature3  feature4  f...  0.506965  0.499143  \n\n[38528 rows x 33 columns], y=0        0\n1        1\n2        1\n3        1\n4   ...\n38526    0\n38527    0\nName: target, dtype: int64, scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True), train=array([ 7634,  7640,  7642, ..., 38525, 38526, 38527]), test=array([   0,    1,    2, ..., 7768, 7769, 7771]), verbose=1, parameters={'learning_rate': 50, 'n_estimators': 250, 'random_state': 42}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1545                              \" numeric value. (Hint: if using 'raise', please\"\n   1546                              \" make sure that it has been spelled correctly.)\"\n   1547                              )\n   1548 \n   1549     else:\n-> 1550         test_score = _score(estimator, X_test, y_test, scorer)\n   1551         if return_train_score:\n   1552             train_score = _score(estimator, X_train, y_train, scorer)\n   1553 \n   1554     scoring_time = time.time() - start_time\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.pyc in _score(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_est...=50,\n          n_estimators=250, random_state=42), X_test=      feature1  feature2  feature3  feature4  fe...1  0.503546  0.480754  \n\n[7706 rows x 33 columns], y_test=0       0\n1       1\n2       1\n3       1\n4       ... 1\n7769    1\n7771    1\nName: target, dtype: int64, scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True))\n   1601 def _score(estimator, X_test, y_test, scorer):\n   1602     \"\"\"Compute the score of an estimator on a given test set.\"\"\"\n   1603     if y_test is None:\n   1604         score = scorer(estimator, X_test)\n   1605     else:\n-> 1606         score = scorer(estimator, X_test, y_test)\n   1607     if not isinstance(score, numbers.Number):\n   1608         raise ValueError(\"scoring must return a number, got %s (%s) instead.\"\n   1609                          % (str(score), type(score)))\n   1610     return score\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/metrics/scorer.pyc in __call__(self=make_scorer(log_loss, greater_is_better=False, needs_proba=True), clf=AdaBoostClassifier(algorithm='SAMME.R', base_est...=50,\n          n_estimators=250, random_state=42), X=      feature1  feature2  feature3  feature4  fe...1  0.503546  0.480754  \n\n[7706 rows x 33 columns], y=0       0\n1       1\n2       1\n3       1\n4       ... 1\n7769    1\n7771    1\nName: target, dtype: int64, sample_weight=None)\n    119         if sample_weight is not None:\n    120             return self._sign * self._score_func(y, y_pred,\n    121                                                  sample_weight=sample_weight,\n    122                                                  **self._kwargs)\n    123         else:\n--> 124             return self._sign * self._score_func(y, y_pred, **self._kwargs)\n    125 \n    126     def _factory_args(self):\n    127         return \", needs_proba=True\"\n    128 \n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.pyc in log_loss(y_true=0       0\n1       1\n2       1\n3       1\n4       ... 1\n7769    1\n7771    1\nName: target, dtype: int64, y_pred=array([[ nan,  nan],\n       [ nan,  nan],\n      ... nan],\n       [ nan,  nan],\n       [ nan,  nan]]), eps=1e-15, normalize=True, sample_weight=None)\n   1557         Y = np.append(1 - Y, Y, axis=1)\n   1558 \n   1559     # Check if dimensions are consistent.\n   1560     check_consistent_length(T, Y)\n   1561     T = check_array(T)\n-> 1562     Y = check_array(Y)\n   1563     if T.shape[1] != Y.shape[1]:\n   1564         raise ValueError(\"y_true and y_pred have different number of classes \"\n   1565                          \"%d, %d\" % (T.shape[1], Y.shape[1]))\n   1566 \n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc in check_array(array=array([[ nan,  nan],\n       [ nan,  nan],\n      ... nan],\n       [ nan,  nan],\n       [ nan,  nan]]), accept_sparse=None, dtype=None, order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None)\n    393             array = array.astype(np.float64)\n    394         if not allow_nd and array.ndim >= 3:\n    395             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n    396                              % (array.ndim, estimator_name))\n    397         if force_all_finite:\n--> 398             _assert_all_finite(array)\n    399 \n    400     shape_repr = _shape_repr(array.shape)\n    401     if ensure_min_samples > 0:\n    402         n_samples = _num_samples(array)\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc in _assert_all_finite(X=array([[ nan,  nan],\n       [ nan,  nan],\n      ... nan],\n       [ nan,  nan],\n       [ nan,  nan]]))\n     49     # everything is finite; fall back to O(n) space np.isfinite to prevent\n     50     # false positives from overflow in sum method.\n     51     if (X.dtype.char in np.typecodes['AllFloat'] and not np.isfinite(X.sum())\n     52             and not np.isfinite(X).all()):\n     53         raise ValueError(\"Input contains NaN, infinity\"\n---> 54                          \" or a value too large for %r.\" % X.dtype)\n     55 \n     56 \n     57 def assert_all_finite(X):\n     58     \"\"\"Throw a ValueError if X contains NaN or infinity.\n\nValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mJoblibValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-48cae6f0dd09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m grid_ada = RandomizedSearchCV(n_iter=10, estimator=model_ada, param_distributions=params_ada, n_jobs=8, \n\u001b[0;32m      3\u001b[0m                               cv=5, refit=True, verbose=1, scoring='log_loss')\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgrid_ada\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_l1_holdout_blend_v1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y_l1_holdout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    994\u001b[0m                                           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m                                           random_state=self.random_state)\n\u001b[1;32m--> 996\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msampled_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.pyc\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, parameter_iterable)\u001b[0m\n\u001b[0;32m    551\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m                                     error_score=self.error_score)\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 for train, test in cv)\n\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    810\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 812\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    813\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    814\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    760\u001b[0m                         \u001b[1;31m# a working pool as they expect.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    761\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize_pool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 762\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    763\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJoblibValueError\u001b[0m: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/runpy.py in _run_module_as_main(mod_name='ipykernel.__main__', alter_argv=1)\n    157     pkg_name = mod_name.rpartition('.')[0]\n    158     main_globals = sys.modules[\"__main__\"].__dict__\n    159     if alter_argv:\n    160         sys.argv[0] = fname\n    161     return _run_code(code, main_globals, None,\n--> 162                      \"__main__\", fname, loader, pkg_name)\n        fname = '/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py'\n        loader = <pkgutil.ImpLoader instance>\n        pkg_name = 'ipykernel'\n    163 \n    164 def run_module(mod_name, init_globals=None,\n    165                run_name=None, alter_sys=False):\n    166     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/runpy.py in _run_code(code=<code object <module> at 0x7f34d7be8eb0, file \"/...2.7/site-packages/ipykernel/__main__.py\", line 1>, run_globals={'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/home/ngergo...python2.7/site-packages/ipykernel/kernelapp.pyc'>}, init_globals=None, mod_name='__main__', mod_fname='/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py', mod_loader=<pkgutil.ImpLoader instance>, pkg_name='ipykernel')\n     67         run_globals.update(init_globals)\n     68     run_globals.update(__name__ = mod_name,\n     69                        __file__ = mod_fname,\n     70                        __loader__ = mod_loader,\n     71                        __package__ = pkg_name)\n---> 72     exec code in run_globals\n        code = <code object <module> at 0x7f34d7be8eb0, file \"/...2.7/site-packages/ipykernel/__main__.py\", line 1>\n        run_globals = {'__builtins__': <module '__builtin__' (built-in)>, '__doc__': None, '__file__': '/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py', '__loader__': <pkgutil.ImpLoader instance>, '__name__': '__main__', '__package__': 'ipykernel', 'app': <module 'ipykernel.kernelapp' from '/home/ngergo...python2.7/site-packages/ipykernel/kernelapp.pyc'>}\n     73     return run_globals\n     74 \n     75 def _run_module_code(code, init_globals=None,\n     76                     mod_name=None, mod_fname=None,\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py in <module>()\n      1 \n      2 \n----> 3 \n      4 if __name__ == '__main__':\n      5     from ipykernel import kernelapp as app\n      6     app.launch_new_instance()\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    587         \n    588         If a global instance already exists, this reinitializes and starts it\n    589         \"\"\"\n    590         app = cls.instance(**kwargs)\n    591         app.initialize(argv)\n--> 592         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    593 \n    594 #-----------------------------------------------------------------------------\n    595 # utility functions, for convenience\n    596 #-----------------------------------------------------------------------------\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    398         \n    399         if self.poller is not None:\n    400             self.poller.start()\n    401         self.kernel.start()\n    402         try:\n--> 403             ioloop.IOLoop.instance().start()\n    404         except KeyboardInterrupt:\n    405             pass\n    406 \n    407 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    146             PollIOLoop.configure(ZMQIOLoop)\n    147         return PollIOLoop.instance()\n    148     \n    149     def start(self):\n    150         try:\n--> 151             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    152         except ZMQError as e:\n    153             if e.errno == ETERM:\n    154                 # quietly return on ETERM\n    155                 pass\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    878                 self._events.update(event_pairs)\n    879                 while self._events:\n    880                     fd, events = self._events.popitem()\n    881                     try:\n    882                         fd_obj, handler_func = self._handlers[fd]\n--> 883                         handler_func(fd_obj, events)\n        handler_func = <function null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 5\n    884                     except (OSError, IOError) as e:\n    885                         if errno_from_exception(e) == errno.EPIPE:\n    886                             # Happens when the client closes the connection\n    887                             pass\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 5), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 5)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=5)\n    428             # dispatch events:\n    429             if events & IOLoop.ERROR:\n    430                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    431                 return\n    432             if events & IOLoop.READ:\n--> 433                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    434                 if not self.socket:\n    435                     return\n    436             if events & IOLoop.WRITE:\n    437                 self._handle_send()\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    460                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    461         else:\n    462             if self._recv_callback:\n    463                 callback = self._recv_callback\n    464                 # self._recv_callback = None\n--> 465                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    466                 \n    467         # self.update_state()\n    468         \n    469 \n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    402         close our socket.\"\"\"\n    403         try:\n    404             # Use a NullContext to ensure that all StackContexts are run\n    405             # inside our blanket exception handler rather than outside.\n    406             with stack_context.NullContext():\n--> 407                 callback(*args, **kwargs)\n        callback = <function null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    408         except:\n    409             gen_log.error(\"Uncaught exception, closing connection.\",\n    410                           exc_info=True)\n    411             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    270         # Fast path when there are no active contexts.\n    271         def null_wrapper(*args, **kwargs):\n    272             try:\n    273                 current_state = _state.contexts\n    274                 _state.contexts = cap_contexts[0]\n--> 275                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    276             finally:\n    277                 _state.contexts = current_state\n    278         null_wrapper._wrapped = True\n    279         return null_wrapper\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    255         if self.control_stream:\n    256             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    257 \n    258         def make_dispatcher(stream):\n    259             def dispatcher(msg):\n--> 260                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    261             return dispatcher\n    262 \n    263         for s in self.shell_streams:\n    264             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {u'allow_stdin': True, u'code': u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2016-09-23T22:06:50.089407', u'msg_id': u'C42EFFEC2ADD4235956D9533428BA7C0', u'msg_type': u'execute_request', u'session': u'1422CF0855EF4AA286CC064870292C16', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'C42EFFEC2ADD4235956D9533428BA7C0', 'msg_type': u'execute_request', 'parent_header': {}})\n    207             self.log.error(\"UNKNOWN MESSAGE TYPE: %r\", msg_type)\n    208         else:\n    209             self.log.debug(\"%s: %s\", msg_type, msg)\n    210             self.pre_handler_hook()\n    211             try:\n--> 212                 handler(stream, idents, msg)\n        handler = <bound method IPythonKernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = ['1422CF0855EF4AA286CC064870292C16']\n        msg = {'buffers': [], 'content': {u'allow_stdin': True, u'code': u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2016-09-23T22:06:50.089407', u'msg_id': u'C42EFFEC2ADD4235956D9533428BA7C0', u'msg_type': u'execute_request', u'session': u'1422CF0855EF4AA286CC064870292C16', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'C42EFFEC2ADD4235956D9533428BA7C0', 'msg_type': u'execute_request', 'parent_header': {}}\n    213             except Exception:\n    214                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    215             finally:\n    216                 self.post_handler_hook()\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=['1422CF0855EF4AA286CC064870292C16'], parent={'buffers': [], 'content': {u'allow_stdin': True, u'code': u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u'silent': False, u'stop_on_error': True, u'store_history': True, u'user_expressions': {}}, 'header': {'date': '2016-09-23T22:06:50.089407', u'msg_id': u'C42EFFEC2ADD4235956D9533428BA7C0', u'msg_type': u'execute_request', u'session': u'1422CF0855EF4AA286CC064870292C16', u'username': u'username', u'version': u'5.0'}, 'metadata': {}, 'msg_id': u'C42EFFEC2ADD4235956D9533428BA7C0', 'msg_type': u'execute_request', 'parent_header': {}})\n    365         if not silent:\n    366             self.execution_count += 1\n    367             self._publish_execute_input(code, parent, self.execution_count)\n    368 \n    369         reply_content = self.do_execute(code, silent, store_history,\n--> 370                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    371 \n    372         # Flush output before sending the reply.\n    373         sys.stdout.flush()\n    374         sys.stderr.flush()\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    170 \n    171         reply_content = {}\n    172         # FIXME: the shell calls the exception handler itself.\n    173         shell._reply_content = None\n    174         try:\n--> 175             shell.run_cell(code, store_history=store_history, silent=silent)\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\"\n        store_history = True\n        silent = False\n    176         except:\n    177             status = u'error'\n    178             # FIXME: this code right now isn't being used yet by default,\n    179             # because the run_cell() call above directly fires off exception\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", store_history=True, silent=False, shell_futures=True)\n   2897                 self.displayhook.exec_result = result\n   2898 \n   2899                 # Execute the user code\n   2900                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2901                 self.run_ast_nodes(code_ast.body, cell_name,\n-> 2902                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler instance>\n   2903 \n   2904                 # Reset this so later displayed values do not modify the\n   2905                 # ExecutionResult\n   2906                 self.displayhook.exec_result = None\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Expr object>], cell_name='<ipython-input-12-48cae6f0dd09>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler instance>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   3007                     return True\n   3008 \n   3009             for i, node in enumerate(to_run_interactive):\n   3010                 mod = ast.Interactive([node])\n   3011                 code = compiler(mod, cell_name, \"single\")\n-> 3012                 if self.run_code(code, result):\n        self.run_code = <bound method ZMQInteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7f348bfd29b0, file \"<ipython-input-12-48cae6f0dd09>\", line 4>\n        result = <IPython.core.interactiveshell.ExecutionResult object>\n   3013                     return True\n   3014 \n   3015             # Flush softspace\n   3016             if softspace(sys.stdout, 0):\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7f348bfd29b0, file \"<ipython-input-12-48cae6f0dd09>\", line 4>, result=<IPython.core.interactiveshell.ExecutionResult object>)\n   3061         outflag = 1  # happens in more places, so it's easier as default\n   3062         try:\n   3063             try:\n   3064                 self.hooks.pre_run_code_hook()\n   3065                 #rprint('Running code', repr(code_obj)) # dbg\n-> 3066                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7f348bfd29b0, file \"<ipython-input-12-48cae6f0dd09>\", line 4>\n        self.user_global_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 'BernoulliNB': <class 'sklearn.naive_bayes.BernoulliNB'>, 'ExtraTreesClassifier': <class 'sklearn.ensemble.forest.ExtraTreesClassifier'>, 'In': ['', u\"train_l1_holdout_blend_v1 = pd.read_csv('Blend...v1 = pd.read_csv('Blend_data/Blend_v1_test.csv')\", u\"# Import libraries\\nimport pandas as pd\\n\\n# m... plt \\nget_ipython().magic(u'matplotlib inline')\", u\"train_l1_holdout_blend_v1 = pd.read_csv('Blend...v1 = pd.read_csv('Blend_data/Blend_v1_test.csv')\", u\"# Determine model parameters: \\nparams_logreg ...            'random_state':[42] \\n             }\", u'# Determine models\\nmodel_logreg = LogisticReg... BernoulliNB()\\nmodel_ada = AdaBoostClassifier()', u\"#Logistic regression\\ngrid_logreg = Randomized..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# XGB\\nsplitter = StratifiedShuffleSplit(y=tra..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"#Logistic regression\\ngrid_logreg = Randomized..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Knn\\ngrid_knn = RandomizedSearchCV(n_iter=10..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Random Forest\\ngrid_rf = RandomizedSearchCV(..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Bernoulli Bayes\\ngrid_naive = RandomizedSear..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\"], 'KNeighborsClassifier': <class 'sklearn.neighbors.classification.KNeighborsClassifier'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {7: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 8: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 9: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 10: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 11: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1)}, 'QuadraticDiscriminantAnalysis': <class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'RandomizedSearchCV': <class 'sklearn.grid_search.RandomizedSearchCV'>, ...}\n        self.user_ns = {'AdaBoostClassifier': <class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'>, 'BernoulliNB': <class 'sklearn.naive_bayes.BernoulliNB'>, 'ExtraTreesClassifier': <class 'sklearn.ensemble.forest.ExtraTreesClassifier'>, 'In': ['', u\"train_l1_holdout_blend_v1 = pd.read_csv('Blend...v1 = pd.read_csv('Blend_data/Blend_v1_test.csv')\", u\"# Import libraries\\nimport pandas as pd\\n\\n# m... plt \\nget_ipython().magic(u'matplotlib inline')\", u\"train_l1_holdout_blend_v1 = pd.read_csv('Blend...v1 = pd.read_csv('Blend_data/Blend_v1_test.csv')\", u\"# Determine model parameters: \\nparams_logreg ...            'random_state':[42] \\n             }\", u'# Determine models\\nmodel_logreg = LogisticReg... BernoulliNB()\\nmodel_ada = AdaBoostClassifier()', u\"#Logistic regression\\ngrid_logreg = Randomized..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# XGB\\nsplitter = StratifiedShuffleSplit(y=tra..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"#Logistic regression\\ngrid_logreg = Randomized..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Knn\\ngrid_knn = RandomizedSearchCV(n_iter=10..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Random Forest\\ngrid_rf = RandomizedSearchCV(..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Bernoulli Bayes\\ngrid_naive = RandomizedSear..._l1_holdout_blend_v1, train_y_l1_holdout.target)\", u\"# Adaboost\\ngrid_ada = RandomizedSearchCV(n_it..._l1_holdout_blend_v1, train_y_l1_holdout.target)\"], 'KNeighborsClassifier': <class 'sklearn.neighbors.classification.KNeighborsClassifier'>, 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'Out': {7: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 8: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 9: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 10: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), 11: RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1)}, 'QuadraticDiscriminantAnalysis': <class 'sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis'>, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'RandomizedSearchCV': <class 'sklearn.grid_search.RandomizedSearchCV'>, ...}\n   3067             finally:\n   3068                 # Reset our crash handler in place\n   3069                 sys.excepthook = old_excepthook\n   3070         except SystemExit as e:\n\n...........................................................................\n/home/ngergoo/Documents/DataMining/Numerai/ROUND_20160922/<ipython-input-12-48cae6f0dd09> in <module>()\n      1 \n      2 \n      3 # Adaboost\n----> 4 grid_ada = RandomizedSearchCV(n_iter=10, estimator=model_ada, param_distributions=params_ada, n_jobs=8, \n      5                               cv=5, refit=True, verbose=1, scoring='log_loss')\n      6 grid_ada.fit(train_l1_holdout_blend_v1, train_y_l1_holdout.target)\n      7 \n      8 \n      9 \n     10 \n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py in fit(self=RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), X=       feature1  feature2  feature3  feature4  f...  0.506965  0.499143  \n\n[38528 rows x 33 columns], y=0        0\n1        1\n2        1\n3        1\n4   ...\n38526    0\n38527    0\nName: target, dtype: int64)\n    991 \n    992         \"\"\"\n    993         sampled_params = ParameterSampler(self.param_distributions,\n    994                                           self.n_iter,\n    995                                           random_state=self.random_state)\n--> 996         return self._fit(X, y, sampled_params)\n        self._fit = <bound method RandomizedSearchCV._fit of Randomi...t=True,\n          scoring='log_loss', verbose=1)>\n        X =        feature1  feature2  feature3  feature4  f...  0.506965  0.499143  \n\n[38528 rows x 33 columns]\n        y = 0        0\n1        1\n2        1\n3        1\n4   ...\n38526    0\n38527    0\nName: target, dtype: int64\n        sampled_params = <sklearn.grid_search.ParameterSampler object>\n    997 \n    998 \n    999 \n   1000 \n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/grid_search.py in _fit(self=RandomizedSearchCV(cv=5, error_score='raise',\n  ...it=True,\n          scoring='log_loss', verbose=1), X=       feature1  feature2  feature3  feature4  f...  0.506965  0.499143  \n\n[38528 rows x 33 columns], y=0        0\n1        1\n2        1\n3        1\n4   ...\n38526    0\n38527    0\nName: target, dtype: int64, parameter_iterable=<sklearn.grid_search.ParameterSampler object>)\n    548         )(\n    549             delayed(_fit_and_score)(clone(base_estimator), X, y, self.scorer_,\n    550                                     train, test, self.verbose, parameters,\n    551                                     self.fit_params, return_parameters=True,\n    552                                     error_score=self.error_score)\n--> 553                 for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.grid_search.ParameterSampler object>\n    554                 for train, test in cv)\n    555 \n    556         # Out is a list of triplet: score, estimator, n_test_samples\n    557         n_fits = len(out)\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=8), iterable=<generator object <genexpr>>)\n    807             if pre_dispatch == \"all\" or n_jobs == 1:\n    808                 # The iterable was consumed all at once by the above for loop.\n    809                 # No need to wait for async callbacks to trigger to\n    810                 # consumption.\n    811                 self._iterating = False\n--> 812             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=8)>\n    813             # Make sure that we get a last message telling us we are done\n    814             elapsed_time = time.time() - self._start_time\n    815             self._print('Done %3i out of %3i | elapsed: %s finished',\n    816                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Sep 23 22:39:04 2016\nPID: 23643                Python 2.7.11: /home/ngergoo/anaconda2/bin/python\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n     67     def __init__(self, iterator_slice):\n     68         self.items = list(iterator_slice)\n     69         self._size = len(self.items)\n     70 \n     71     def __call__(self):\n---> 72         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n     73 \n     74     def __len__(self):\n     75         return self._size\n     76 \n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.pyc in _fit_and_score(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_est...=50,\n          n_estimators=250, random_state=42), X=       feature1  feature2  feature3  feature4  f...  0.506965  0.499143  \n\n[38528 rows x 33 columns], y=0        0\n1        1\n2        1\n3        1\n4   ...\n38526    0\n38527    0\nName: target, dtype: int64, scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True), train=array([ 7634,  7640,  7642, ..., 38525, 38526, 38527]), test=array([   0,    1,    2, ..., 7768, 7769, 7771]), verbose=1, parameters={'learning_rate': 50, 'n_estimators': 250, 'random_state': 42}, fit_params={}, return_train_score=False, return_parameters=True, error_score='raise')\n   1545                              \" numeric value. (Hint: if using 'raise', please\"\n   1546                              \" make sure that it has been spelled correctly.)\"\n   1547                              )\n   1548 \n   1549     else:\n-> 1550         test_score = _score(estimator, X_test, y_test, scorer)\n   1551         if return_train_score:\n   1552             train_score = _score(estimator, X_train, y_train, scorer)\n   1553 \n   1554     scoring_time = time.time() - start_time\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/cross_validation.pyc in _score(estimator=AdaBoostClassifier(algorithm='SAMME.R', base_est...=50,\n          n_estimators=250, random_state=42), X_test=      feature1  feature2  feature3  feature4  fe...1  0.503546  0.480754  \n\n[7706 rows x 33 columns], y_test=0       0\n1       1\n2       1\n3       1\n4       ... 1\n7769    1\n7771    1\nName: target, dtype: int64, scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True))\n   1601 def _score(estimator, X_test, y_test, scorer):\n   1602     \"\"\"Compute the score of an estimator on a given test set.\"\"\"\n   1603     if y_test is None:\n   1604         score = scorer(estimator, X_test)\n   1605     else:\n-> 1606         score = scorer(estimator, X_test, y_test)\n   1607     if not isinstance(score, numbers.Number):\n   1608         raise ValueError(\"scoring must return a number, got %s (%s) instead.\"\n   1609                          % (str(score), type(score)))\n   1610     return score\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/metrics/scorer.pyc in __call__(self=make_scorer(log_loss, greater_is_better=False, needs_proba=True), clf=AdaBoostClassifier(algorithm='SAMME.R', base_est...=50,\n          n_estimators=250, random_state=42), X=      feature1  feature2  feature3  feature4  fe...1  0.503546  0.480754  \n\n[7706 rows x 33 columns], y=0       0\n1       1\n2       1\n3       1\n4       ... 1\n7769    1\n7771    1\nName: target, dtype: int64, sample_weight=None)\n    119         if sample_weight is not None:\n    120             return self._sign * self._score_func(y, y_pred,\n    121                                                  sample_weight=sample_weight,\n    122                                                  **self._kwargs)\n    123         else:\n--> 124             return self._sign * self._score_func(y, y_pred, **self._kwargs)\n    125 \n    126     def _factory_args(self):\n    127         return \", needs_proba=True\"\n    128 \n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/metrics/classification.pyc in log_loss(y_true=0       0\n1       1\n2       1\n3       1\n4       ... 1\n7769    1\n7771    1\nName: target, dtype: int64, y_pred=array([[ nan,  nan],\n       [ nan,  nan],\n      ... nan],\n       [ nan,  nan],\n       [ nan,  nan]]), eps=1e-15, normalize=True, sample_weight=None)\n   1557         Y = np.append(1 - Y, Y, axis=1)\n   1558 \n   1559     # Check if dimensions are consistent.\n   1560     check_consistent_length(T, Y)\n   1561     T = check_array(T)\n-> 1562     Y = check_array(Y)\n   1563     if T.shape[1] != Y.shape[1]:\n   1564         raise ValueError(\"y_true and y_pred have different number of classes \"\n   1565                          \"%d, %d\" % (T.shape[1], Y.shape[1]))\n   1566 \n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc in check_array(array=array([[ nan,  nan],\n       [ nan,  nan],\n      ... nan],\n       [ nan,  nan],\n       [ nan,  nan]]), accept_sparse=None, dtype=None, order=None, copy=False, force_all_finite=True, ensure_2d=True, allow_nd=False, ensure_min_samples=1, ensure_min_features=1, warn_on_dtype=False, estimator=None)\n    393             array = array.astype(np.float64)\n    394         if not allow_nd and array.ndim >= 3:\n    395             raise ValueError(\"Found array with dim %d. %s expected <= 2.\"\n    396                              % (array.ndim, estimator_name))\n    397         if force_all_finite:\n--> 398             _assert_all_finite(array)\n    399 \n    400     shape_repr = _shape_repr(array.shape)\n    401     if ensure_min_samples > 0:\n    402         n_samples = _num_samples(array)\n\n...........................................................................\n/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/utils/validation.pyc in _assert_all_finite(X=array([[ nan,  nan],\n       [ nan,  nan],\n      ... nan],\n       [ nan,  nan],\n       [ nan,  nan]]))\n     49     # everything is finite; fall back to O(n) space np.isfinite to prevent\n     50     # false positives from overflow in sum method.\n     51     if (X.dtype.char in np.typecodes['AllFloat'] and not np.isfinite(X.sum())\n     52             and not np.isfinite(X).all()):\n     53         raise ValueError(\"Input contains NaN, infinity\"\n---> 54                          \" or a value too large for %r.\" % X.dtype)\n     55 \n     56 \n     57 def assert_all_finite(X):\n     58     \"\"\"Throw a ValueError if X contains NaN or infinity.\n\nValueError: Input contains NaN, infinity or a value too large for dtype('float64').\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "# Adaboost - gives back some weird value error, about nans/infinite values in df X?!\n",
    "grid_ada = RandomizedSearchCV(n_iter=10, estimator=model_ada, param_distributions=params_ada, n_jobs=8, \n",
    "                              cv=5, refit=True, verbose=1, scoring='log_loss')\n",
    "grid_ada.fit(train_l1_holdout_blend_v1, train_y_l1_holdout.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed: 10.1min\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed: 19.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
       "          fit_params={'eval_set': [(       feature1  feature2  feature3  feature4  feature5  feature6  feature7  \\\n",
       "0      0.695981  0.596473  0.515205  0.884092  0.320731  0.546269  0.095469\n",
       "1      0.676988  0.644239  0.444708  0.558028  0.221347  0.092506  0.229707\n",
       "2      0.387228  0.662142  0.639957  ...    1\n",
       "11557    1\n",
       "11558    0\n",
       "Name: target, dtype: int64)], 'early_stopping_rounds': 25, 'verbose': 0},\n",
       "          iid=True, n_iter=20, n_jobs=2,\n",
       "          param_distributions={'n_estimators': [250, 500, 1000, 1500, 2000, 3000, 4000, 5000], 'subsample': [0.2, 0.5, 0.7, 1], 'reg_lambda': [1, 2, 5, 10], 'seed': [42], 'silent': [1], 'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.3], 'nthread': [4], 'gamma': [0, 0.0001, 0.001, 0.01, 0.1], 'max_depth': [1, 5, 10, 15, 20, 25]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='log_loss', verbose=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGB\n",
    "splitter = StratifiedShuffleSplit(y=train_y_l1_holdout, n_iter=1, train_size=0.7, \n",
    "                                      test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, test_index in splitter: \n",
    "    train_1 = train_l1_holdout_blend_v1.iloc[train_index,:]\n",
    "    train_1.reset_index(drop=True, inplace=True)\n",
    "    train_y_1 = train_y_l1_holdout.iloc[train_index,:]\n",
    "    train_y_1.reset_index(drop=True, inplace=True)\n",
    "    train_2 = train_l1_holdout_blend_v1.iloc[test_index,:]\n",
    "    train_2.reset_index(drop=True, inplace=True)\n",
    "    train_y_2 = train_y_l1_holdout.iloc[test_index,:]\n",
    "    train_y_2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "xgb_fit_params = {'eval_set':[(train_1, train_y_1.target),(train_2, train_y_2.target)], \n",
    "                  'early_stopping_rounds':25, 'verbose':0}\n",
    "\n",
    "grid_xgb = RandomizedSearchCV(n_iter=20, estimator=model_xgb, param_distributions=params_xgb, \n",
    "                              n_jobs=2, cv=5, refit=True, verbose=1, scoring='log_loss', \n",
    "                              fit_params=xgb_fit_params\n",
    "                             )\n",
    "\n",
    "grid_xgb.fit(train_l1_holdout_blend_v1, train_y_l1_holdout.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logreg:  {'fit_intercept': True, 'C': 0.1, 'n_jobs': 4, 'verbose': 0, 'solver': 'lbfgs', 'max_iter': 500, 'random_state': 42, 'tol': 0.001} -0.69152967219\n",
      "KNN:  {'p': 1, 'n_jobs': 4, 'leaf_size': 40, 'algorithm': 'brute', 'n_neighbors': 3000} -0.69168230852\n",
      "RandomForest:  {'oob_score': True, 'n_jobs': 4, 'verbose': 0, 'min_samples_leaf': 25, 'n_estimators': 4000, 'max_features': 5, 'random_state': 42, 'criterion': 'gini', 'min_samples_split': 1, 'max_depth': 10} -0.691457346439\n",
      "Bernoulli bayes:  {'binarize': 10, 'alpha': 80, 'fit_prior': False} -0.692724946458\n",
      "Xgboost:  {'silent': 1, 'learning_rate': 0.1, 'nthread': 4, 'n_estimators': 2000, 'subsample': 1, 'reg_lambda': 10, 'seed': 42, 'max_depth': 1, 'gamma': 0} -0.691545945181\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "print 'Logreg: ', grid_logreg.best_params_, grid_logreg.best_score_\n",
    "print 'KNN: ', grid_knn.best_params_, grid_knn.best_score_\n",
    "print 'RandomForest: ', grid_rf.best_params_, grid_rf.best_score_\n",
    "print 'Bernoulli bayes: ', grid_naive.best_params_, grid_naive.best_score_\n",
    "# print 'Adaboost: ', grid_ada.best_params_, grid_ada.best_score_\n",
    "print 'Xgboost: ', grid_xgb.best_params_, grid_xgb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(pd.DataFrame(grid_logreg.grid_scores_), 'GridResults/blender_logreg.csv', index=False)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(grid_knn.grid_scores_), 'GridResults/blender_knn.csv', index=False)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(grid_rf.grid_scores_), 'GridResults/blender_rf.csv', index=False)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(grid_naive.grid_scores_), 'GridResults/blender_naive.csv', index=False)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(grid_xgb.grid_scores_), 'GridResults/blender_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- Make submission by using each individual layer1 model\n",
    "    - Try to add more non-correlated predictions for layer1\n",
    "- Make submission by using single blender model\n",
    "- Make submission by using average of multiple blender model\n",
    "    - Try to add more non-correlated blender models\n",
    "- Make prediction by using layer1 predictions only. \n",
    "\n",
    "- Grid search for blender by using predictions only\n",
    "- Grid search for blender by using predictions and basic stats only\n",
    "\n",
    "\n",
    "Although these results looks promising, I bet, that there is a trick with feature engineering. \n",
    "\n",
    "As the next step, try to do similarity thing, and go through my notes to get some ideas. \n",
    "\n",
    "\n",
    "## Grid search for blender model with predictions only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_l1_holdout_blend_v1 = pd.read_csv('Blend_data/Blend_v1_train_l1_holdout.csv')\n",
    "train_y_l1_holdout = pd.read_csv('Blend_data/Blend_v1_train_y_holdout.csv')\n",
    "\n",
    "test_blend_v1 = pd.read_csv('Blend_data/Blend_v1_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_l1_holdout_blend_v1 = train_l1_holdout_blend_v1.iloc[:,-6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Determine model parameters: \n",
    "params_logreg = {'n_jobs':[4],\n",
    "                 'C':[0.0001,0.1,0.5,1.5,2,5,10], \n",
    "                 'fit_intercept':[False,True], \n",
    "                 'max_iter':[10,100,500,1000,2000],\n",
    "                 'solver':['newton-cg','lbfgs','liblinear','sag'],\n",
    "                 'tol':[0.00001,0.0001,0.001,0.1,0.5],\n",
    "                 'verbose':[0],\n",
    "                 'random_state':[42]}\n",
    "\n",
    "params_knn = {'n_jobs':[4],\n",
    "              'n_neighbors':[5,50,100,500,1000,1500,2000,3000,4000,5000], \n",
    "              'p':[1,2,3,4,5,10,11,12,15,20],\n",
    "              'leaf_size':[10,20,30,40,50,60],\n",
    "              'algorithm':['auto','ball_tree','kd_tree','brute']}\n",
    "\n",
    "# care for max_features\n",
    "params_rf = {'n_jobs':[4], \n",
    "             'criterion':['gini', 'entropy'],\n",
    "             'n_estimators':[250,500,1000,1500,2000,3000,4000,5000], \n",
    "             'max_features':[1,2,3,4,5,6], \n",
    "             'max_depth':[1,5,10,15,20,25,50,100], \n",
    "             'min_samples_split':[1,5,10,25,50], \n",
    "             'min_samples_leaf':[1,5,10,25,50], \n",
    "             'oob_score':[True,False], \n",
    "             'verbose':[0], \n",
    "             'random_state':[42]}\n",
    "\n",
    "params_xgb = {'silent':[1],\n",
    "              'nthread':[4], \n",
    "              'seed':[42], \n",
    "              'max_depth':[1,5,10,15,20,25],\n",
    "              'subsample':[0.2,0.5,0.7,1],\n",
    "              'reg_lambda':[1,2,5,10],\n",
    "              'learning_rate':[0.0001,0.001,0.01,0.1,0.3], \n",
    "              'gamma':[0,0.0001,0.001,0.01,0.1],\n",
    "              'n_estimators':[250,500,1000,1500,2000,3000,4000,5000]\n",
    "             }\n",
    "\n",
    "params_naive = {'alpha':[0.1,0.5,0.75,1,1.5,2,5,10,15,25,30,40,50,60,70,80,90,100,200,500], \n",
    "                'binarize':[0.1,0.5,0.75,1,1.5,2,5,10,15,25,30,40,50,60,70,80,90,100,200,500],\n",
    "                'fit_prior':[True, False], \n",
    "               }\n",
    "\n",
    "params_ada = {'n_estimators':[250,500,1000,1500,2000,3000,4000,5000],\n",
    "              'learning_rate':[0.0001,0.001,0.01,0.1,0.5,0.75,1.5,2,5,10,25,50], \n",
    "              'random_state':[42] \n",
    "             }\n",
    "\n",
    "# Determine models\n",
    "model_logreg = LogisticRegression()\n",
    "model_knn = KNeighborsClassifier()\n",
    "model_rf = RandomForestClassifier()\n",
    "model_xgb = xgb.XGBClassifier()\n",
    "model_naive = BernoulliNB()\n",
    "model_ada = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 500 candidates, totalling 2500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=2)]: Done 196 tasks      | elapsed:   22.0s\n",
      "[Parallel(n_jobs=2)]: Done 446 tasks      | elapsed:   49.3s\n",
      "[Parallel(n_jobs=2)]: Done 796 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=2)]: Done 1246 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=2)]: Done 1796 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=2)]: Done 2446 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=2)]: Done 2500 out of 2500 | elapsed:  4.9min finished\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:1207: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for (class_, warm_start_coef_) in zip(classes_, warm_start_coef))\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:1207: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for (class_, warm_start_coef_) in zip(classes_, warm_start_coef))\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/sag.py:267: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/utils/optimize.py:200: UserWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"newton-cg failed to converge. Increase the \"\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/utils/optimize.py:200: UserWarning: newton-cg failed to converge. Increase the number of iterations.\n",
      "  warnings.warn(\"newton-cg failed to converge. Increase the \"\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/scipy/optimize/linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/scipy/optimize/linesearch.py:285: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/utils/optimize.py:193: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/utils/optimize.py:193: UserWarning: Line Search failed\n",
      "  warnings.warn('Line Search failed')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False),\n",
       "          fit_params={}, iid=True, n_iter=500, n_jobs=2,\n",
       "          param_distributions={'C': [0.0001, 0.1, 0.5, 1.5, 2, 5, 10], 'n_jobs': [4], 'verbose': [0], 'tol': [1e-05, 0.0001, 0.001, 0.1, 0.5], 'fit_intercept': [False, True], 'random_state': [42], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag'], 'max_iter': [10, 100, 500, 1000, 2000]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='log_loss', verbose=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Logistic regression\n",
    "grid_logreg = RandomizedSearchCV(n_iter = 500, estimator=model_logreg, param_distributions=params_logreg, \n",
    "                                 n_jobs=2, cv=5, refit=True, verbose=1, scoring='log_loss')\n",
    "\n",
    "grid_logreg.fit(train_l1_holdout_blend_v1, train_y_l1_holdout.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=2)]: Done  50 out of  50 | elapsed:  6.5min finished\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:1060: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for s in gen_even_slices(Y.shape[0], n_jobs))\n",
      "/home/ngergoo/anaconda2/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:1060: UserWarning: Multiprocessing-backed parallel loops cannot be nested, setting n_jobs=1\n",
      "  for s in gen_even_slices(Y.shape[0], n_jobs))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=2,\n",
       "          param_distributions={'n_neighbors': [5, 50, 100, 500, 1000, 1500, 2000, 3000, 4000, 5000], 'n_jobs': [4], 'leaf_size': [10, 20, 30, 40, 50, 60], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], 'p': [1, 2, 3, 4, 5, 10, 11, 12, 15, 20]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='log_loss', verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Knn\n",
    "grid_knn = RandomizedSearchCV(n_iter=10, estimator=model_knn, param_distributions=params_knn, n_jobs=2, cv=5, \n",
    "                             refit=True, verbose=1, scoring='log_loss')\n",
    "\n",
    "grid_knn.fit(train_l1_holdout_blend_v1, train_y_l1_holdout.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed: 16.8min\n",
      "[Parallel(n_jobs=2)]: Done  50 out of  50 | elapsed: 17.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False),\n",
       "          fit_params={}, iid=True, n_iter=10, n_jobs=2,\n",
       "          param_distributions={'n_estimators': [250, 500, 1000, 1500, 2000, 3000, 4000, 5000], 'min_samples_split': [1, 5, 10, 25, 50], 'oob_score': [True, False], 'n_jobs': [4], 'criterion': ['gini', 'entropy'], 'verbose': [0], 'max_features': [1, 2, 3, 4, 5, 6], 'random_state': [42], 'max_depth': [1, 5, 10, 15, 20, 25, 50, 100], 'min_samples_leaf': [1, 5, 10, 25, 50]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='log_loss', verbose=1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "grid_rf = RandomizedSearchCV(estimator=model_rf, param_distributions=params_rf, n_jobs=2, cv=5, refit=True, \n",
    "                             verbose=1, scoring='log_loss')\n",
    "\n",
    "grid_rf.fit(train_l1_holdout_blend_v1, train_y_l1_holdout.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=8)]: Done  88 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=8)]: Done 200 out of 200 | elapsed:    1.7s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True),\n",
       "          fit_params={}, iid=True, n_iter=40, n_jobs=8,\n",
       "          param_distributions={'binarize': [0.1, 0.5, 0.75, 1, 1.5, 2, 5, 10, 15, 25, 30, 40, 50, 60, 70, 80, 90, 100, 200, 500], 'alpha': [0.1, 0.5, 0.75, 1, 1.5, 2, 5, 10, 15, 25, 30, 40, 50, 60, 70, 80, 90, 100, 200, 500], 'fit_prior': [True, False]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='log_loss', verbose=1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bernoulli Bayes\n",
    "grid_naive = RandomizedSearchCV(n_iter=40, estimator=model_naive, param_distributions=params_naive, n_jobs=8, \n",
    "                                cv=5, refit=True, verbose=1, scoring='log_loss')\n",
    "grid_naive.fit(train_l1_holdout_blend_v1, train_y_l1_holdout.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=2)]: Done  46 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=2)]: Done 100 out of 100 | elapsed:  4.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5, error_score='raise',\n",
       "          estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
       "          fit_params={'eval_set': [(       pred_logreg  pred_knn   pred_rf  pred_naive  pred_ada  pred_xgb\n",
       "0         0.520858   0.52000  0.535538    0.504673  0.505963  0.524817\n",
       "1         0.478932   0.51325  0.504731    0.504673  0.512173  0.490121\n",
       "2         0.539671   0.52250  0.521396    0.504673  0.512173 ...    1\n",
       "11557    1\n",
       "11558    0\n",
       "Name: target, dtype: int64)], 'early_stopping_rounds': 25, 'verbose': 0},\n",
       "          iid=True, n_iter=20, n_jobs=2,\n",
       "          param_distributions={'n_estimators': [250, 500, 1000, 1500, 2000, 3000, 4000, 5000], 'subsample': [0.2, 0.5, 0.7, 1], 'reg_lambda': [1, 2, 5, 10], 'seed': [42], 'silent': [1], 'learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.3], 'nthread': [4], 'gamma': [0, 0.0001, 0.001, 0.01, 0.1], 'max_depth': [1, 5, 10, 15, 20, 25]},\n",
       "          pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "          scoring='log_loss', verbose=1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGB\n",
    "splitter = StratifiedShuffleSplit(y=train_y_l1_holdout, n_iter=1, train_size=0.7, \n",
    "                                      test_size=0.3, random_state=42)\n",
    "\n",
    "for train_index, test_index in splitter: \n",
    "    train_1 = train_l1_holdout_blend_v1.iloc[train_index,:]\n",
    "    train_1.reset_index(drop=True, inplace=True)\n",
    "    train_y_1 = train_y_l1_holdout.iloc[train_index,:]\n",
    "    train_y_1.reset_index(drop=True, inplace=True)\n",
    "    train_2 = train_l1_holdout_blend_v1.iloc[test_index,:]\n",
    "    train_2.reset_index(drop=True, inplace=True)\n",
    "    train_y_2 = train_y_l1_holdout.iloc[test_index,:]\n",
    "    train_y_2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "xgb_fit_params = {'eval_set':[(train_1, train_y_1.target),(train_2, train_y_2.target)], \n",
    "                  'early_stopping_rounds':25, 'verbose':0}\n",
    "\n",
    "grid_xgb = RandomizedSearchCV(n_iter=20, estimator=model_xgb, param_distributions=params_xgb, \n",
    "                              n_jobs=2, cv=5, refit=True, verbose=1, scoring='log_loss', \n",
    "                              fit_params=xgb_fit_params\n",
    "                             )\n",
    "\n",
    "grid_xgb.fit(train_l1_holdout_blend_v1, train_y_l1_holdout.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adaboost - gives back some weird value error, about nans/infinite values in df X?!\n",
    "grid_ada = RandomizedSearchCV(n_iter=10, estimator=model_ada, param_distributions=params_ada, n_jobs=8, \n",
    "                              cv=5, refit=True, verbose=1, scoring='log_loss')\n",
    "grid_ada.fit(train_l1_holdout_blend_v1, train_y_l1_holdout.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logreg:  {'fit_intercept': True, 'C': 1.5, 'n_jobs': 4, 'verbose': 0, 'solver': 'lbfgs', 'max_iter': 500, 'random_state': 42, 'tol': 0.0001} -0.691378634761\n",
      "KNN:  {'p': 2, 'n_jobs': 4, 'leaf_size': 50, 'algorithm': 'ball_tree', 'n_neighbors': 4000} -0.69173965538\n",
      "RandomForest:  {'oob_score': False, 'n_jobs': 4, 'verbose': 0, 'min_samples_leaf': 1, 'n_estimators': 3000, 'max_features': 1, 'random_state': 42, 'criterion': 'gini', 'min_samples_split': 1, 'max_depth': 5} -0.691632767865\n",
      "Bernoulli bayes:  {'binarize': 0.75, 'alpha': 0.5, 'fit_prior': True} -0.693093824732\n",
      "Xgboost:  {'silent': 1, 'learning_rate': 0.01, 'nthread': 4, 'n_estimators': 2000, 'subsample': 0.7, 'reg_lambda': 10, 'seed': 42, 'max_depth': 5, 'gamma': 0} -0.691921694662\n"
     ]
    }
   ],
   "source": [
    "# results\n",
    "print 'Logreg: ', grid_logreg.best_params_, grid_logreg.best_score_\n",
    "print 'KNN: ', grid_knn.best_params_, grid_knn.best_score_\n",
    "print 'RandomForest: ', grid_rf.best_params_, grid_rf.best_score_\n",
    "print 'Bernoulli bayes: ', grid_naive.best_params_, grid_naive.best_score_\n",
    "# print 'Adaboost: ', grid_ada.best_params_, grid_ada.best_score_\n",
    "print 'Xgboost: ', grid_xgb.best_params_, grid_xgb.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.DataFrame.to_csv(pd.DataFrame(grid_logreg.grid_scores_), 'GridResults/blender_predonly_logreg.csv', \n",
    "                    index=False)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(grid_knn.grid_scores_), 'GridResults/blender_predonly_knn.csv', index=False)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(grid_rf.grid_scores_), 'GridResults/blender_predonly_rf.csv', index=False)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(grid_naive.grid_scores_), 'GridResults/blender_predonly_naive.csv', \n",
    "                    index=False)\n",
    "pd.DataFrame.to_csv(pd.DataFrame(grid_xgb.grid_scores_), 'GridResults/blender_predonly_xgb.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
